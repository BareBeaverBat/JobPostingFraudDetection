{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.preprocessing as preproc\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from DataNamesReference import *\n",
    "from LstmTraining import getNonEmptyLines\n",
    "\n",
    "CSV_READ_ARGS = {\"keep_default_na\":False, \"index_col\":0, \"dtype\":COLUMN_DATA_TYPES}\n",
    "\n",
    "MAX_TITLE_LEN = 8\n",
    "MAX_LOCATION_LEN =5\n",
    "MAX_DEPARTMENT_LEN = 3\n",
    "MAX_COMPANY_PROFILE_LEN = 200\n",
    "MAX_DESCRIPTION_LEN = 300\n",
    "MAX_REQUIREMENTS_LEN=200\n",
    "MAX_BENEFITS_LEN = 125\n",
    "\n",
    "PADDING_TYPE = \"post\"\n",
    "TRUNCATING_TYPE = \"post\"\n",
    "\n",
    "#not eliminating any of the options\n",
    "TITLE_VOCAB_SIZE=4708 + 1\n",
    "LOCATION_VOCAB_SIZE=2335+1\n",
    "DEPARTMENT_VOCAB_SIZE=1060+1\n",
    "COMPANY_PROFILE_VOCAB_SIZE=13527+1\n",
    "DESCRIPTION_VOCAB_SIZE=33470+1\n",
    "REQUIREMENTS_VOCAB_SIZE=25259+1\n",
    "BENEFITS_VOCAB_SIZE=11717+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EMBED_DIM = 100 # todo try boosting up to 200 or 300\n",
    "LSTM_SIZE=150\n",
    "\n",
    "BASE_EMBED_DROPOUT= 0.2\n",
    "BASE_LSTM_DROPOUT=0.2\n",
    "BASE_DENSE_DROPOUT=0.2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "trainDataDf = pd.read_csv(TRAIN_DATA_PATH, **CSV_READ_ARGS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#do final preprocessing on each text attrib\n",
    "def convertTitlesToPaddedSequences(dataDf):\n",
    "    allTitles = getNonEmptyLines(TITLES_SUMMARY_FILE_PATH)\n",
    "    titleTokenizer = preproc.text.Tokenizer(num_words=TITLE_VOCAB_SIZE)\n",
    "    titleTokenizer.fit_on_texts(allTitles)\n",
    "\n",
    "    trainTitles = dataDf[TITLE_LABEL]\n",
    "    trainTitleSequences = titleTokenizer.texts_to_sequences(trainTitles)\n",
    "    paddedTrainTitleSequences = preproc.sequence.pad_sequences(trainTitleSequences, maxlen=MAX_TITLE_LEN,\n",
    "                                                               padding=PADDING_TYPE, truncating=TRUNCATING_TYPE)\n",
    "\n",
    "    return paddedTrainTitleSequences\n",
    "\n",
    "def convertLocationsToPaddedSequences(dataDf):\n",
    "    allLocations = getNonEmptyLines(LOCATIONS_SUMMARY_FILE_PATH)\n",
    "    locationTokenizer = preproc.text.Tokenizer(num_words=LOCATION_VOCAB_SIZE)\n",
    "    locationTokenizer.fit_on_texts(allLocations)\n",
    "\n",
    "    trainLocations = dataDf[LOCATION_LABEL]\n",
    "    trainLocationSequences = locationTokenizer.texts_to_sequences(trainLocations)\n",
    "    paddedTrainLocationSequences = preproc.sequence.pad_sequences(trainLocationSequences, maxlen=MAX_LOCATION_LEN,\n",
    "                                                               padding=PADDING_TYPE, truncating=TRUNCATING_TYPE)\n",
    "\n",
    "    return paddedTrainLocationSequences\n",
    "\n",
    "def convertDepartmentsToPaddedSequences(dataDf):\n",
    "    allDepartments = getNonEmptyLines(DEPARTMENTS_SUMMARY_FILE_PATH)\n",
    "    departmentTokenizer = preproc.text.Tokenizer(num_words=DEPARTMENT_VOCAB_SIZE)\n",
    "    departmentTokenizer.fit_on_texts(allDepartments)\n",
    "\n",
    "    trainDepartments = dataDf[DEPARTMENT_LABEL]\n",
    "    trainDepartmentSequences = departmentTokenizer.texts_to_sequences(trainDepartments)\n",
    "    paddedTrainDepartmentSequences = preproc.sequence.pad_sequences(trainDepartmentSequences, maxlen=MAX_DEPARTMENT_LEN,\n",
    "                                                               padding=PADDING_TYPE, truncating=TRUNCATING_TYPE)\n",
    "\n",
    "    return paddedTrainDepartmentSequences\n",
    "\n",
    "def convertCompanyProfilesToPaddedSequences(dataDf):\n",
    "    allCompanyProfiles = getNonEmptyLines(COMPANY_PROFILES_SUMMARY_FILE_PATH)\n",
    "    companyProfileTokenizer = preproc.text.Tokenizer(num_words=COMPANY_PROFILE_VOCAB_SIZE)\n",
    "    companyProfileTokenizer.fit_on_texts(allCompanyProfiles)\n",
    "\n",
    "    trainCompanyProfiles = dataDf[COMPANY_PROFILE_LABEL]\n",
    "    trainCompanyProfileSequences = companyProfileTokenizer.texts_to_sequences(trainCompanyProfiles)\n",
    "    paddedTrainCompanyProfileSequences = preproc.sequence.pad_sequences(trainCompanyProfileSequences, maxlen=MAX_COMPANY_PROFILE_LEN,\n",
    "                                                               padding=PADDING_TYPE, truncating=TRUNCATING_TYPE)\n",
    "\n",
    "    return paddedTrainCompanyProfileSequences\n",
    "\n",
    "def convertDescriptionsToPaddedSequences(dataDf):\n",
    "    allDescriptions = getNonEmptyLines(DESCRIPTIONS_SUMMARY_FILE_PATH)\n",
    "    descriptionTokenizer = preproc.text.Tokenizer(num_words=DESCRIPTION_VOCAB_SIZE)\n",
    "    descriptionTokenizer.fit_on_texts(allDescriptions)\n",
    "\n",
    "    trainDescriptions = dataDf[DESCRIPTION_LABEL]\n",
    "    trainDescriptionSequences = descriptionTokenizer.texts_to_sequences(trainDescriptions)\n",
    "    paddedTrainDescriptionSequences = preproc.sequence.pad_sequences(trainDescriptionSequences, maxlen=MAX_DESCRIPTION_LEN,\n",
    "                                                               padding=PADDING_TYPE, truncating=TRUNCATING_TYPE)\n",
    "\n",
    "    return paddedTrainDescriptionSequences\n",
    "\n",
    "def convertRequirementsToPaddedSequences(dataDf):\n",
    "    allRequirements = getNonEmptyLines(REQUIREMENTS_SUMMARY_FILE_PATH)\n",
    "    requirementsTokenizer = preproc.text.Tokenizer(num_words=REQUIREMENTS_VOCAB_SIZE)\n",
    "    requirementsTokenizer.fit_on_texts(allRequirements)\n",
    "\n",
    "    trainRequirements = dataDf[REQUIREMENTS_LABEL]\n",
    "    trainRequirementsSequences = requirementsTokenizer.texts_to_sequences(trainRequirements)\n",
    "    paddedTrainRequirementsSequences = preproc.sequence.pad_sequences(trainRequirementsSequences, maxlen=MAX_REQUIREMENTS_LEN,\n",
    "                                                               padding=PADDING_TYPE, truncating=TRUNCATING_TYPE)\n",
    "\n",
    "    return paddedTrainRequirementsSequences\n",
    "\n",
    "def convertBenefitsToPaddedSequences(dataDf):\n",
    "    allBenefits = getNonEmptyLines(BENEFITS_SUMMARY_FILE_PATH)\n",
    "    benefitsTokenizer = preproc.text.Tokenizer(num_words=BENEFITS_VOCAB_SIZE)\n",
    "    benefitsTokenizer.fit_on_texts(allBenefits)\n",
    "\n",
    "    trainBenefits = dataDf[BENEFITS_LABEL]\n",
    "    trainBenefitsSequences = benefitsTokenizer.texts_to_sequences(trainBenefits)\n",
    "    paddedTrainBenefitsSequences = preproc.sequence.pad_sequences(trainBenefitsSequences, maxlen=MAX_BENEFITS_LEN,\n",
    "                                                               padding=PADDING_TYPE, truncating=TRUNCATING_TYPE)\n",
    "\n",
    "    return paddedTrainBenefitsSequences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "trainTitleSequences = convertTitlesToPaddedSequences(trainDataDf)\n",
    "trainLocationSequences = convertLocationsToPaddedSequences(trainDataDf)\n",
    "trainDepartmentSequences = convertDepartmentsToPaddedSequences(trainDataDf)\n",
    "trainCompanyProfileSequences = convertCompanyProfilesToPaddedSequences(trainDataDf)\n",
    "trainDescriptionSequences = convertDescriptionsToPaddedSequences(trainDataDf)\n",
    "trainRequirementsSequences = convertRequirementsToPaddedSequences(trainDataDf)\n",
    "trainBenefitsSequences = convertBenefitsToPaddedSequences(trainDataDf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "trainLabels = trainDataDf[FRAUDULENT_LABEL]\n",
    "trainLabels = trainLabels.astype(float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#todo implement use of word2vec pretrained embedding matrix\n",
    "\n",
    "#no recurrent_dropout on lstm because need to use GPU\n",
    "\n",
    "#todo build lstm model for a text attrib\n",
    "descriptionInputLayer = layers.Input(shape=(MAX_DESCRIPTION_LEN,), dtype=\"int32\")\n",
    "\n",
    "descriptionEmbedLayer = layers.Embedding(input_dim=DESCRIPTION_VOCAB_SIZE, output_dim=EMBED_DIM,\n",
    "                                         mask_zero=True, input_length=MAX_DESCRIPTION_LEN)\n",
    "descriptionEmbedOutput = descriptionEmbedLayer(descriptionInputLayer)\n",
    "\n",
    "descriptionLstmLayer = layers.LSTM(units=LSTM_SIZE, dropout=BASE_LSTM_DROPOUT)\n",
    "descriptionLstmOutput = descriptionLstmLayer(descriptionEmbedOutput)\n",
    "\n",
    "#secondary model output to allow for better training of the description-specific lstm\n",
    "descriptionSidePredLayer = layers.Dense(1, activation=\"sigmoid\")\n",
    "descriptionSidePrediction = descriptionSidePredLayer(descriptionLstmOutput)\n",
    "\n",
    "#todo insert batchnormalization layers?\n",
    "\n",
    "fraudModel = Model(inputs=[descriptionInputLayer], outputs=[descriptionSidePrediction])\n",
    "\n",
    "\n",
    "#todo build mlp model for some categ attribs\n",
    "\n",
    "#todo combine component models into final output model\n",
    "\n",
    "#todo add extra model output for each lstm in order to improve their training\n",
    "\n",
    "#todo add weighting for positive examples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#todo train on training data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#todo evaluate on validation data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}